{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Project with Hugging Face - Complete Implementation\n",
    "# Sentiment Analysis on IMDb Movie Reviews\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BERTSentimentAnalyzer:\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", max_length=512):\n",
    "        \"\"\"\n",
    "        Initialize BERT Sentiment Analyzer\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Hugging Face model name\n",
    "            max_length (int): Maximum sequence length for tokenization\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.trainer = None\n",
    "        self.training_logs = []\n",
    "        \n",
    "    def load_model_and_tokenizer(self):\n",
    "        \"\"\"Load pre-trained BERT model and tokenizer\"\"\"\n",
    "        print(f\"Loading model: {self.model_name}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name, \n",
    "            num_labels=2,  # Binary classification (positive/negative)\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "        \n",
    "        # Add padding token if not present\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        print(\"Model and tokenizer loaded successfully!\")\n",
    "        \n",
    "    def prepare_dataset(self, dataset_name=\"imdb\", sample_size=None):\n",
    "        \"\"\"\n",
    "        Load and prepare the IMDb dataset\n",
    "        \n",
    "        Args:\n",
    "            dataset_name (str): Dataset name from Hugging Face\n",
    "            sample_size (int): Optional sample size for faster training\n",
    "        \"\"\"\n",
    "        print(f\"Loading {dataset_name} dataset...\")\n",
    "        \n",
    "        # Load dataset\n",
    "        dataset = load_dataset(dataset_name)\n",
    "        \n",
    "        # Sample dataset if specified (for faster experimentation)\n",
    "        if sample_size:\n",
    "            train_dataset = dataset['train'].shuffle(seed=42).select(range(sample_size))\n",
    "            test_dataset = dataset['test'].shuffle(seed=42).select(range(sample_size // 4))\n",
    "        else:\n",
    "            train_dataset = dataset['train']\n",
    "            test_dataset = dataset['test']\n",
    "            \n",
    "        # Split training set for validation\n",
    "        train_test_split = train_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        self.train_dataset = train_test_split['train']\n",
    "        self.val_dataset = train_test_split['test']\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "        print(f\"Dataset loaded - Train: {len(self.train_dataset)}, \"\n",
    "              f\"Val: {len(self.val_dataset)}, Test: {len(self.test_dataset)}\")\n",
    "        \n",
    "    def tokenize_function(self, examples):\n",
    "        \"\"\"Tokenize text data\"\"\"\n",
    "        return self.tokenizer(\n",
    "            examples['text'],\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    \n",
    "    def prepare_data_for_training(self):\n",
    "        \"\"\"Tokenize datasets for training\"\"\"\n",
    "        print(\"Tokenizing datasets...\")\n",
    "        \n",
    "        self.train_dataset = self.train_dataset.map(self.tokenize_function, batched=True)\n",
    "        self.val_dataset = self.val_dataset.map(self.tokenize_function, batched=True)\n",
    "        self.test_dataset = self.test_dataset.map(self.tokenize_function, batched=True)\n",
    "        \n",
    "        # Set format for PyTorch\n",
    "        self.train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "        self.val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "        self.test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "        \n",
    "        print(\"Tokenization completed!\")\n",
    "        \n",
    "    def compute_metrics(self, eval_pred):\n",
    "        \"\"\"Compute evaluation metrics\"\"\"\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "    \n",
    "    def setup_training(self, output_dir=\"./results\", **kwargs):\n",
    "        \"\"\"\n",
    "        Set up training arguments and trainer\n",
    "        \n",
    "        Args:\n",
    "            output_dir (str): Directory to save model and logs\n",
    "            **kwargs: Additional training arguments\n",
    "        \"\"\"\n",
    "        # Default training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=kwargs.get('num_train_epochs', 3),\n",
    "            per_device_train_batch_size=kwargs.get('batch_size', 16),\n",
    "            per_device_eval_batch_size=kwargs.get('batch_size', 16),\n",
    "            warmup_steps=kwargs.get('warmup_steps', 500),\n",
    "            weight_decay=kwargs.get('weight_decay', 0.01),\n",
    "            learning_rate=kwargs.get('learning_rate', 2e-5),\n",
    "            logging_dir=f'{output_dir}/logs',\n",
    "            logging_steps=kwargs.get('logging_steps', 100),\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=kwargs.get('eval_steps', 500),\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=kwargs.get('save_steps', 500),\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            greater_is_better=True,\n",
    "            report_to=None,  # Disable wandb/tensorboard\n",
    "            save_total_limit=2,\n",
    "            dataloader_num_workers=0,  # Avoid multiprocessing issues\n",
    "        )\n",
    "        \n",
    "        # Set up trainer\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=self.train_dataset,\n",
    "            eval_dataset=self.val_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "        )\n",
    "        \n",
    "        print(\"Training setup completed!\")\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        print(\"Starting training...\")\n",
    "        \n",
    "        # Train the model\n",
    "        train_result = self.trainer.train()\n",
    "        \n",
    "        # Save the model\n",
    "        self.trainer.save_model()\n",
    "        \n",
    "        # Store training logs\n",
    "        self.training_logs = self.trainer.state.log_history\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        return train_result\n",
    "    \n",
    "    def evaluate_model(self, dataset_type=\"test\"):\n",
    "        \"\"\"\n",
    "        Evaluate the model on specified dataset\n",
    "        \n",
    "        Args:\n",
    "            dataset_type (str): \"test\", \"val\", or \"train\"\n",
    "        \"\"\"\n",
    "        if dataset_type == \"test\":\n",
    "            dataset = self.test_dataset\n",
    "        elif dataset_type == \"val\":\n",
    "            dataset = self.val_dataset\n",
    "        else:\n",
    "            dataset = self.train_dataset\n",
    "            \n",
    "        print(f\"Evaluating on {dataset_type} dataset...\")\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = self.trainer.predict(dataset)\n",
    "        \n",
    "        # Extract metrics\n",
    "        metrics = predictions.metrics\n",
    "        \n",
    "        # Get predicted labels\n",
    "        predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "        true_labels = predictions.label_ids\n",
    "        \n",
    "        # Print detailed results\n",
    "        print(f\"\\n{dataset_type.upper()} RESULTS:\")\n",
    "        print(f\"Accuracy: {metrics[f'test_accuracy']:.4f}\")\n",
    "        print(f\"F1-Score: {metrics[f'test_f1']:.4f}\")\n",
    "        print(f\"Precision: {metrics[f'test_precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics[f'test_recall']:.4f}\")\n",
    "        \n",
    "        return metrics, predicted_labels, true_labels\n",
    "    \n",
    "    def plot_confusion_matrix(self, true_labels, predicted_labels, title=\"Confusion Matrix\"):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        cm = confusion_matrix(true_labels, predicted_labels)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=['Negative', 'Positive'],\n",
    "                   yticklabels=['Negative', 'Positive'])\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if not self.training_logs:\n",
    "            print(\"No training logs available!\")\n",
    "            return\n",
    "            \n",
    "        # Extract metrics\n",
    "        train_loss = []\n",
    "        eval_loss = []\n",
    "        eval_f1 = []\n",
    "        steps = []\n",
    "        \n",
    "        for log in self.training_logs:\n",
    "            if 'loss' in log:\n",
    "                train_loss.append(log['loss'])\n",
    "                steps.append(log['step'])\n",
    "            if 'eval_loss' in log:\n",
    "                eval_loss.append(log['eval_loss'])\n",
    "                eval_f1.append(log['eval_f1'])\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot loss\n",
    "        ax1.plot(steps, train_loss, label='Training Loss', color='blue')\n",
    "        if eval_loss:\n",
    "            eval_steps = steps[-len(eval_loss):]\n",
    "            ax1.plot(eval_steps, eval_loss, label='Validation Loss', color='red')\n",
    "        ax1.set_xlabel('Steps')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Plot F1 score\n",
    "        if eval_f1:\n",
    "            eval_steps = steps[-len(eval_f1):]\n",
    "            ax2.plot(eval_steps, eval_f1, label='Validation F1', color='green')\n",
    "            ax2.set_xlabel('Steps')\n",
    "            ax2.set_ylabel('F1 Score')\n",
    "            ax2.set_title('Validation F1 Score')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def predict_sentiment(self, texts):\n",
    "        \"\"\"\n",
    "        Predict sentiment for new texts\n",
    "        \n",
    "        Args:\n",
    "            texts (list): List of text strings to classify\n",
    "        \"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "            \n",
    "        # Tokenize texts\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Get predictions\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            \n",
    "        # Convert to labels\n",
    "        predicted_labels = torch.argmax(predictions, dim=-1)\n",
    "        confidence_scores = torch.max(predictions, dim=-1)[0]\n",
    "        \n",
    "        results = []\n",
    "        for i, text in enumerate(texts):\n",
    "            sentiment = \"Positive\" if predicted_labels[i] == 1 else \"Negative\"\n",
    "            confidence = confidence_scores[i].item()\n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'sentiment': sentiment,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def debug_model_performance(self, threshold=0.8):\n",
    "        \"\"\"\n",
    "        Debug model performance and suggest improvements\n",
    "        \n",
    "        Args:\n",
    "            threshold (float): Accuracy threshold for \"good\" performance\n",
    "        \"\"\"\n",
    "        print(\"üîç DEBUGGING MODEL PERFORMANCE\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_metrics, val_pred, val_true = self.evaluate_model(\"val\")\n",
    "        val_accuracy = val_metrics['test_accuracy']\n",
    "        \n",
    "        print(f\"Current Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        if val_accuracy < threshold:\n",
    "            print(f\"‚ö†Ô∏è  Performance below threshold ({threshold})\")\n",
    "            print(\"\\nüõ†Ô∏è  DEBUGGING SUGGESTIONS:\")\n",
    "            \n",
    "            if val_accuracy < 0.6:\n",
    "                print(\"1. CRITICAL: Very low accuracy detected!\")\n",
    "                print(\"   - Check data preprocessing and tokenization\")\n",
    "                print(\"   - Verify label encoding (0/1 for binary classification)\")\n",
    "                print(\"   - Consider using a smaller learning rate (1e-5)\")\n",
    "                print(\"   - Increase training epochs\")\n",
    "                \n",
    "            elif val_accuracy < 0.75:\n",
    "                print(\"1. MODERATE: Below average performance\")\n",
    "                print(\"   - Try different model variants (distilbert, roberta)\")\n",
    "                print(\"   - Adjust learning rate and batch size\")\n",
    "                print(\"   - Implement data augmentation\")\n",
    "                print(\"   - Check for class imbalance\")\n",
    "                \n",
    "            else:\n",
    "                print(\"1. MINOR: Close to threshold\")\n",
    "                print(\"   - Fine-tune hyperparameters\")\n",
    "                print(\"   - Increase training data\")\n",
    "                print(\"   - Try ensemble methods\")\n",
    "                \n",
    "            # Check for overfitting\n",
    "            train_metrics, _, _ = self.evaluate_model(\"train\")\n",
    "            train_accuracy = train_metrics['test_accuracy']\n",
    "            \n",
    "            if train_accuracy - val_accuracy > 0.1:\n",
    "                print(\"\\n2. OVERFITTING DETECTED!\")\n",
    "                print(\"   - Add dropout or regularization\")\n",
    "                print(\"   - Reduce model complexity\")\n",
    "                print(\"   - Use early stopping\")\n",
    "                print(\"   - Increase validation data\")\n",
    "                \n",
    "        else:\n",
    "            print(\"‚úÖ Model performance looks good!\")\n",
    "            \n",
    "        print(\"\\nüìä DETAILED METRICS:\")\n",
    "        for metric, value in val_metrics.items():\n",
    "            if metric.startswith('test_'):\n",
    "                clean_metric = metric.replace('test_', '').title()\n",
    "                print(f\"   {clean_metric}: {value:.4f}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the complete BERT project\"\"\"\n",
    "    print(\"üöÄ BERT SENTIMENT ANALYSIS PROJECT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = BERTSentimentAnalyzer(model_name=\"bert-base-uncased\")\n",
    "    \n",
    "    try:\n",
    "        # Part 1: Fine-tuning BERT\n",
    "        print(\"\\nüìã PART 1: FINE-TUNING BERT\")\n",
    "        analyzer.load_model_and_tokenizer()\n",
    "        analyzer.prepare_dataset(sample_size=5000)  # Use sample for faster training\n",
    "        analyzer.prepare_data_for_training()\n",
    "        \n",
    "        # Setup training with optimized parameters\n",
    "        analyzer.setup_training(\n",
    "            output_dir=\"./bert_sentiment_model\",\n",
    "            num_train_epochs=3,\n",
    "            batch_size=16,\n",
    "            learning_rate=2e-5,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_steps=100,\n",
    "            eval_steps=500,\n",
    "            save_steps=500\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        train_result = analyzer.train()\n",
    "        \n",
    "        # Part 2: Debugging Issues\n",
    "        print(\"\\nüîß PART 2: DEBUGGING ISSUES\")\n",
    "        analyzer.debug_model_performance(threshold=0.85)\n",
    "        \n",
    "        # Part 3: Evaluating the Model\n",
    "        print(\"\\nüìä PART 3: MODEL EVALUATION\")\n",
    "        test_metrics, test_pred, test_true = analyzer.evaluate_model(\"test\")\n",
    "        \n",
    "        # Plot results\n",
    "        analyzer.plot_training_history()\n",
    "        analyzer.plot_confusion_matrix(test_true, test_pred, \"Test Set Confusion Matrix\")\n",
    "        \n",
    "        # Part 4: Creative Application\n",
    "        print(\"\\nüé® PART 4: CREATIVE APPLICATION\")\n",
    "        \n",
    "        # Test on custom examples\n",
    "        sample_texts = [\n",
    "            \"This movie was absolutely fantastic! I loved every minute of it.\",\n",
    "            \"Terrible film, complete waste of time. Very disappointing.\",\n",
    "            \"The movie was okay, nothing special but not bad either.\",\n",
    "            \"Outstanding performance by the actors, brilliant cinematography!\",\n",
    "            \"I fell asleep halfway through. Boring and predictable.\"\n",
    "        ]\n",
    "        \n",
    "        predictions = analyzer.predict_sentiment(sample_texts)\n",
    "        \n",
    "        print(\"Sample Predictions:\")\n",
    "        for pred in predictions:\n",
    "            print(f\"Text: {pred['text'][:50]}...\")\n",
    "            print(f\"Sentiment: {pred['sentiment']} (Confidence: {pred['confidence']:.3f})\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "        # Final Summary\n",
    "        print(\"\\nüéØ PROJECT SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"‚úÖ Model trained successfully\")\n",
    "        print(f\"‚úÖ Final Test Accuracy: {test_metrics['test_accuracy']:.4f}\")\n",
    "        print(f\"‚úÖ Final Test F1-Score: {test_metrics['test_f1']:.4f}\")\n",
    "        print(f\"‚úÖ Model saved to: ./bert_sentiment_model\")\n",
    "        \n",
    "        # Advanced techniques summary\n",
    "        print(f\"\\nüî¨ TECHNIQUES USED:\")\n",
    "        print(f\"   ‚Ä¢ Pre-trained BERT (bert-base-uncased)\")\n",
    "        print(f\"   ‚Ä¢ Fine-tuning with Hugging Face Trainer\")\n",
    "        print(f\"   ‚Ä¢ Early stopping to prevent overfitting\")\n",
    "        print(f\"   ‚Ä¢ Learning rate scheduling with warmup\")\n",
    "        print(f\"   ‚Ä¢ Weight decay for regularization\")\n",
    "        print(f\"   ‚Ä¢ Comprehensive evaluation metrics\")\n",
    "        print(f\"   ‚Ä¢ Performance debugging and optimization\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error occurred: {str(e)}\")\n",
    "        logger.error(f\"Training failed: {str(e)}\")\n",
    "        \n",
    "        # Debugging suggestions for common errors\n",
    "        print(\"\\nüõ†Ô∏è  COMMON ISSUES AND SOLUTIONS:\")\n",
    "        print(\"1. CUDA out of memory: Reduce batch_size to 8 or 4\")\n",
    "        print(\"2. Dataset loading issues: Check internet connection\")\n",
    "        print(\"3. Tokenization errors: Verify model_name is correct\")\n",
    "        print(\"4. Training crashes: Try reducing max_length to 256\")\n",
    "\n",
    "# Additional utility functions for advanced users\n",
    "class AdvancedBERTFeatures:\n",
    "    \"\"\"Advanced features for BERT fine-tuning\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def data_augmentation(texts, labels, augment_factor=2):\n",
    "        \"\"\"\n",
    "        Simple data augmentation by adding synonym replacement\n",
    "        Note: This is a placeholder - implement proper augmentation\n",
    "        \"\"\"\n",
    "        augmented_texts = []\n",
    "        augmented_labels = []\n",
    "        \n",
    "        for text, label in zip(texts, labels):\n",
    "            augmented_texts.append(text)\n",
    "            augmented_labels.append(label)\n",
    "            \n",
    "            # Simple augmentation (in practice, use libraries like nlpaug)\n",
    "            if len(text.split()) > 5:  # Only augment longer texts\n",
    "                words = text.split()\n",
    "                # Simple word shuffling (replace with proper synonym replacement)\n",
    "                if len(words) > 3:\n",
    "                    shuffled = words[:-1] + [words[-1]]  # Keep last word\n",
    "                    augmented_text = ' '.join(shuffled)\n",
    "                    augmented_texts.append(augmented_text)\n",
    "                    augmented_labels.append(label)\n",
    "                    \n",
    "        return augmented_texts[:len(texts) * augment_factor], augmented_labels[:len(labels) * augment_factor]\n",
    "    \n",
    "    @staticmethod\n",
    "    def ensemble_predictions(models, texts, tokenizer):\n",
    "        \"\"\"\n",
    "        Ensemble predictions from multiple models\n",
    "        \n",
    "        Args:\n",
    "            models (list): List of trained models\n",
    "            texts (list): Input texts\n",
    "            tokenizer: Tokenizer for preprocessing\n",
    "        \"\"\"\n",
    "        all_predictions = []\n",
    "        \n",
    "        for model in models:\n",
    "            inputs = tokenizer(texts, truncation=True, padding=True, return_tensors='pt')\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                all_predictions.append(predictions)\n",
    "        \n",
    "        # Average predictions\n",
    "        ensemble_pred = torch.mean(torch.stack(all_predictions), dim=0)\n",
    "        return torch.argmax(ensemble_pred, dim=-1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
